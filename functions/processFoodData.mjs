// ‚úÖ Import required modules
import fs from "fs"; // File system module to read JSON dataset
import path from "path"; // Helps resolve absolute file paths
import { fileURLToPath } from "url"; // Converts module URL to file path
import { Pinecone } from "@pinecone-database/pinecone"; // Pinecone vector database
import { OpenAIEmbeddings } from "@langchain/openai"; // OpenAI embeddings model
import dotenv from "dotenv"; // Loads environment variables

// ‚úÖ Resolve the absolute path to `.env` file
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const envPath = path.resolve(__dirname, "..", ".env");

// ‚úÖ Load environment variables using the absolute path
dotenv.config({ path: envPath });

// ‚úÖ Debugging logs for environment variables
console.log("üìå Loading .env from:", envPath);
console.log("üìå File exists:", fs.existsSync(envPath));
console.log("üîç Pinecone API Key:", process.env.PINECONE_API_KEY ? "‚úÖ Loaded" : "‚ùå Not found");
console.log("üîç Pinecone Index Name:", process.env.PINECONE_INDEX_NAME ? "‚úÖ Loaded" : "‚ùå Not found");


// ‚úÖ Initialize Pinecone client
// The Pinecone Node.js SDK (v6.x) is designed for async operations.
// No special pooling or thread configuration is needed here; async/await handles concurrency implicitly.
const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });

async function loadFoodData() {
    // ‚úÖ Verify API key and index name are loaded correctly from .env
    if (!process.env.PINECONE_API_KEY) {
        throw new Error("‚ùå Pinecone API key is missing! Check your .env file.");
    }
    if (!process.env.PINECONE_INDEX_NAME) {
        throw new Error("‚ùå Pinecone index name is missing! Check your .env file.");
    }

    // ‚úÖ Connect to Pinecone index
    // This creates an index client instance that points to your 'meal-insights' index.
    const index = pinecone.index(process.env.PINECONE_INDEX_NAME);
    const namespace = "nutrition"; // Namespace for storing meal insights. You can use this to logically partition your data.

    // ‚úÖ Read the food dataset file
    const jsonPath = path.resolve(__dirname, "food_nutrients_fixed.json");
    console.log("üìå Loading dataset from:", jsonPath);
    console.log("üìå File exists:", fs.existsSync(jsonPath));

    // Read the raw JSON data and parse it into a JavaScript array of objects.
    const rawData = fs.readFileSync(jsonPath, "utf-8");
    const foodItems = JSON.parse(rawData); // Convert JSON text into JavaScript objects

    // ‚úÖ Debug: Log total items count to confirm data loading
    console.log(`üî¢ Total food items to process: ${foodItems.length}`);

    // ‚úÖ Prepare vectorized embeddings
    // We will collect all generated vectors in this array first.
    const allVectors = [];

    // Loop through each food item to generate its embedding
    for (let i = 0; i < foodItems.length; i++) {
        const item = foodItems[i];

        // ‚úÖ Ensure item has a valid name for the text description
        // If 'item.name' is missing or empty, assign a default name for better readability in logs/embeddings.
        const itemName = item.name ? item.name.trim() : `Unknown Dish ${i}`;

        // ‚úÖ Construct a descriptive text string from the food item's nutritional data.
        // This text will be used to generate the embedding.
        const text = `${itemName}: ${item.total_calories || 0} calories, ${item.total_fat || 0}g fat, ${item.total_carb || 0}g carbs, ${item.total_protein || 0}g protein`;

        // ‚úÖ Generate OpenAI embeddings for the constructed text.
        // The `embedQuery` method sends the text to OpenAI and returns a numerical vector.
        try {
            const embedding = await new OpenAIEmbeddings().embedQuery(text);

            // ‚úÖ Store vectorized data in Pinecone's expected format.
            // Each object in the `vectors` array needs an 'id', 'values' (the embedding), and optionally 'metadata'.
            allVectors.push({
                id: item.id || `auto_${i}`, // Use existing item ID or generate a unique one.
                values: embedding, // The dense vector generated by OpenAI.
                metadata: {
                    text: text, // Store the original text for context/retrieval later.
                    name: itemName,
                    calories: item.total_calories || 0,
                    fat: item.total_fat || 0,
                    carb: item.total_carb || 0,
                    protein: item.total_protein || 0,
                    // You can add more metadata fields as needed from your JSON.
                },
            });

            // ‚úÖ Log progress for the first few items to confirm processing
            if (i < 5) {
                console.log(`‚úÖ Processed item ${i + 1} of ${foodItems.length}: ${itemName}`);
            }

            // ‚úÖ Show periodic progress updates for large datasets
            if ((i + 1) % 500 === 0) { // Log every 500 items
                console.log(`üîÑ Progress: Processed ${i + 1} items so far...`);
            }
        } catch (err) {
            // ‚úÖ Error handling for embedding generation.
            // If an embedding fails, log the error and skip this item to avoid crashing the whole process.
            console.error(`‚ö†Ô∏è Error processing embedding for item ${i} (${itemName}):`, err);
            continue; // Move to the next item
        }
    }

    // ‚úÖ Pre-upload check: Ensure that we actually have vectors to upload.
    // This helps catch issues where no embeddings were generated (e.g., API key problem).
    if (!Array.isArray(allVectors) || allVectors.length === 0) {
        throw new Error("‚ùå No valid vectors found for upload after processing. Check OpenAI API key and data format.");
    }

    // --- START DEBUGGING ADDITIONS (Confirming structure before upsert) ---
    console.log("\n--- Debugging Pinecone Upsert Payload ---");
    console.log("Type of 'allVectors' variable:", typeof allVectors);
    console.log("Is 'allVectors' an array?", Array.isArray(allVectors));
    console.log("Number of vectors prepared:", allVectors.length);

    if (allVectors.length > 0) {
        console.log("Structure of first vector:", JSON.stringify(allVectors[0], null, 2));
        console.log("Type of values in first vector:", typeof allVectors[0].values);
        console.log("Length of values in first vector:", allVectors[0].values?.length);
        // Additional check: Ensure 'values' is an array of numbers and has correct dimension
        if (Array.isArray(allVectors[0].values) && allVectors[0].values.length === 1536 && typeof allVectors[0].values[0] === 'number') {
            console.log("Values array looks correct (1536 dimensions, numbers).");
        } else {
            console.warn("‚ö†Ô∏è Warning: Values array in first vector might not be correctly formatted or dimension is off.");
        }
    }
    console.log("--- End Debugging ---");
    // --- END DEBUGGING ADDITIONS ---

    // ‚úÖ Implement Batching for Pinecone Upsert
    // For large datasets, sending all vectors in a single request can lead to timeouts or payload limits.
    // We split the `allVectors` array into smaller batches and upload them sequentially.
    // Pinecone recommends batch sizes up to 1000 vectors, but smaller batches (e.g., 100-200)
    // are often more robust for very large embeddings or complex metadata.
    const batchSize = 100; // Adjusted for robustness with OpenAI's 1536-dimension embeddings.
    console.log(`üöÄ Starting Pinecone upsert in batches of ${batchSize} vectors...`);

    let uploadedCount = 0;
    // Iterate through `allVectors` array, taking `batchSize` elements at a time.
    for (let i = 0; i < allVectors.length; i += batchSize) {
        const batch = allVectors.slice(i, i + batchSize); // Extract the current batch of vectors.
        try {
            // ‚úÖ Upload the current batch of data to Pinecone.
            await index.upsert({ vectors: batch, namespace });
            uploadedCount += batch.length; // Increment count of successfully uploaded vectors.
            console.log(
                `‚úÖ Uploaded batch ${Math.floor(i / batchSize) + 1} of ${Math.ceil(allVectors.length / batchSize)} ` +
                `(${batch.length} vectors). Total uploaded: ${uploadedCount}`
            );
        } catch (batchError) {
            // ‚úÖ Detailed error handling for each batch.
            // If a batch fails, log the error but try to continue with subsequent batches.
            // For production, you might want more sophisticated retry logic or error reporting.
            console.error(`‚ùå Error uploading batch starting at index ${i}:`, batchError);
            console.error(`‚ùå Batch details (first item):`, JSON.stringify(batch[0], null, 2));
            // Depending on the nature of the error, you might choose to `throw batchError;`
            // to stop the entire process if the error is critical and unrecoverable.
            // For now, we'll continue to the next batch.
            continue;
        }
    }

    console.log(`üéâ ‚úÖ Food data successfully loaded into Pinecone! Total vectors uploaded: ${uploadedCount}`);
}

// ‚úÖ Run the main function with a global error handler.
// This catches any unhandled exceptions that might occur during the entire process.
loadFoodData().catch((err) => console.error("‚ùå An unhandled error occurred during data loading:", err));